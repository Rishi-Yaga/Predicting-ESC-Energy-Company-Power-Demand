---
title: "IDS_Full_Code"
output: html_document
date: "2023-12-06"
---

```{r}
# Load the required libraries
library(arrow)       # Library for reading parquet files
library(dplyr)       # Library for data manipulation
library(tidyverse)   # Tidyverse for data processing
library(caret)       # Library for machine learning and data splitting
library(ggplot2)     # For Visualizations
```


####################################### Data Merging ###########################################


```{r}
# Read the static house data
static_house_info = read_parquet("https://intro-datascience.s3.us-east-2.amazonaws.com/SC-data/static_house_info.parquet")

# Create list of all house IDs
house_id <- static_house_info$bldg_id

```

```{r}
# Initialize a storage structure for the full data frame
full_df <- list()  # List to store data frames for each house
n <- 0  # Counter for tracking progress

# Loop through each house ID
for (i in house_id) {
  # Construct the URL for the energy data
  energy_url <- paste0("https://intro-datascience.s3.us-east-2.amazonaws.com/SC-data/2023-houseData/", i, ".parquet")
  
  tryCatch({
    # Read parquet file
    energy <- read_parquet(energy_url)
    
    # Filter data for July 2018
    energy <- energy[format(energy$time, "%Y-%m") == "2018-07", ]
 
    # Calculate total energy consumed
    energy$total_energy_consumed <- rowSums(energy[, 1:42], na.rm = TRUE)
    
    # Add house ID to the data
    energy$bldg_id <- i
    
    # Merge with static house information
    energy_house_merge <- left_join(energy, static_house_info, by = 'bldg_id')
    
    # Merge with weather data
    county <- energy_house_merge$in.county[1]
    weather_url <- paste0("https://intro-datascience.s3.us-east-2.amazonaws.com/SC-data/weather/2023-weather-data/", county, ".csv")
    weather <- read_csv(weather_url)
    weather$in.county <- county
    energy_house_weather_merge <- energy_house_merge %>% left_join(weather, by = c('in.county', 'time' = 'date_time'))
    
    # Append to the full data frame list
    full_df[[length(full_df) + 1]] <- energy_house_weather_merge
  }, error = function(e) {
    # Handle errors during data processing
    message(paste("Error processing house ID", i, ":", e$message))
  })
  
  # Increment counter and check progress
  n <- n + 1
  print(paste("Processed house ID", i, ":", n, "of", length(house_id)))
}

```

```{r}
# Combine all data frames into one
data_merged <- bind_rows(full_df)
```

```{r}
# Save the data frame into a parquet file
write_parquet(data_merged, "/Users/mj/Desktop/IDS_Nikitha/data_merged.parquet")
```


####################################### Data Summarization by day ###########################################

```{r}

# Read the parquet file into 'data_merged'
data_merged <- read_parquet("/Users/mj/Desktop/IDS_Nikitha/data_merged.parquet")

# Convert the 'time' column to a Date object and create a new 'day' column
data_merged$day <- as.Date(data_merged$time, format = "%Y-%m-%d %H:%M:%S")

# Group and summarize the data
data_summarized <- data_merged %>%
  group_by(bldg_id, time, in.county) %>%
  summarize(
    total_energy_consumed = sum(total_energy_consumed, na.rm = TRUE),
    total_energy_produced = sum(out.electricity.pv.energy_consumption, na.rm = TRUE),
    
    total_Direct_Normal_Radiation = sum(`Direct Normal Radiation [W/m2]`, na.rm = TRUE),
    total_Diffuse_Horizontal_Radiation = sum(`Diffuse Horizontal Radiation [W/m2]`, na.rm = TRUE),
    median_Dry_Bulb_Temperature = median(`Dry Bulb Temperature [°C]`, na.rm = TRUE),
    median_Relative_Humidity = median(`Relative Humidity [%]`, na.rm = TRUE),
    median_Wind_Speed = median(`Wind Speed [m/s]`, na.rm = TRUE),
    median_Wind_Direction = median(`Wind Direction [Deg]`, na.rm = TRUE),
    total_Global_Horizontal_Radiation = sum(`Global Horizontal Radiation [W/m2]`, na.rm = TRUE),
    .groups = 'drop'  )

# Understand the data
summary(data_summarized)

```

```{r}
# Read the static house data
static_house_info <- read_parquet("https://intro-datascience.s3.us-east-2.amazonaws.com/SC-data/static_house_info.parquet")

# Merge data_summarized with static_house_info based on "bldg_id"
data_summarized <- merge(data_summarized, static_house_info, by = "bldg_id")

```

```{r}
# Save the data frame into a parquet file
write_parquet(data_summarized, "/Users/mj/Desktop/IDS_Nikitha/data_summarized.parquet")
```

####################################### Data Cleaning ###########################################

```{r}
# Read the parquet file into data_summarized
data_summarized <- read_parquet("/Users/mj/Desktop/IDS_Nikitha/data_summarized.parquet")

# Create a copy for using in the next steps
data_cleaned <- data_summarized

```

```{r}
# Remove Columns with Zero Variance
data_cleaned <- data_cleaned[, sapply(data_cleaned, function(x) length(unique(x)) > 1)]
```

```{r}
# Define a function to convert income range strings to mean values
range_to_mean <- function(range_str) {
  # Handle cases where the value is expressed as ">X" (e.g., ">100000")
  if (grepl(">", range_str)) {
    # Interpret ">X" as "X+1" to find a mean value
    return(as.numeric(gsub(">", "", range_str)) + 1)
  }
  
  # Handle cases where the value is expressed as "<X" (e.g., "<3000")
  if (grepl("<", range_str)) {
    # Interpret "<X" as "X-1" to find a mean value
    return(as.numeric(gsub("<", "", range_str)) - 1)
  }

  # Split the string on the hyphen to handle ranges
  parts <- strsplit(range_str, "-")[[1]]
  
  # Remove any '+' signs and convert the parts to numeric
  parts <- as.numeric(gsub("\\+", "", parts))
  
  # Calculate the mean of the two numbers if a range is present
  if (length(parts) == 2) {
    return(mean(parts))
  } else {
    # If there's no range, return the single value
    return(parts[1])
  }
}

# Apply the range_to_mean function to specific columns in data_cleaned
data_cleaned$in.income <- sapply(data_cleaned$in.income, range_to_mean)
data_cleaned$in.income_recs_2015 <- sapply(data_cleaned$in.income_recs_2015, range_to_mean)
data_cleaned$in.income_recs_2020 <- sapply(data_cleaned$in.income_recs_2020, range_to_mean)

```

```{r}
# Convert the 'in.refrigerator' column to numeric
data_cleaned$in.refrigerator <- gsub("EF |, 100% Usage", "", data_cleaned$in.refrigerator)
data_cleaned$in.refrigerator[data_cleaned$in.refrigerator == "None"] <- "0"
data_cleaned$in.refrigerator <- as.numeric(data_cleaned$in.refrigerator)
```

```{r}
# Convert in. variables from factor to numeric to use in prediction
data_cleaned$in.bathroom_spot_vent_hour <- as.numeric(sub("Hour", "", data_cleaned$in.bathroom_spot_vent_hour))

data_cleaned$in.range_spot_vent_hour <- as.numeric(sub("Hour", "", data_cleaned$in.range_spot_vent_hour))

data_cleaned$in.cooling_setpoint <- as.numeric(sub("F", "", data_cleaned$in.cooling_setpoint))

data_cleaned$in.cooling_setpoint_offset_magnitude <- as.numeric(sub("F", "", data_cleaned$in.cooling_setpoint_offset_magnitude))

data_cleaned$in.heating_setpoint <- as.numeric(sub("F", "", data_cleaned$in.heating_setpoint))

data_cleaned$in.heating_setpoint_offset_magnitude <- as.numeric(sub("F", "", data_cleaned$in.heating_setpoint_offset_magnitude))

data_cleaned$in.infiltration <- as.numeric(sub(" ACH50", "", data_cleaned$in.infiltration))

data_cleaned$in.infiltration <- as.numeric(sub(" ACH50", "", data_cleaned$in.infiltration))

data_cleaned$in.geometry_garage <- ifelse(data_cleaned$in.geometry_garage == "None", 0, as.numeric(gsub(" Car", "", data_cleaned$in.geometry_garage)))

```

```{r}
# Create a new column 'energy_usage_group' based on these thresholds for classification

#summary(data_cleaned$total_energy_consumed)

low_threshold <- 19.80
medium_threshold <- 35.74
high_threshold <- 122.49

data_cleaned$energy_usage_group <- dplyr::case_when(
  data_cleaned$total_energy_consumed < low_threshold ~ 'Low',
  data_cleaned$total_energy_consumed >= low_threshold & data_cleaned$total_energy_consumed < medium_threshold ~ 'Medium',
  data_cleaned$total_energy_consumed >= medium_threshold & data_cleaned$total_energy_consumed < high_threshold ~ 'High',
  data_cleaned$total_energy_consumed >= high_threshold ~ 'Very High',
  TRUE ~ 'Unknown'
)

# Create a new column 'building_size' based on these thresholds for classification

#summary(data_cleaned$in.sqft)

data_cleaned$building_size <- cut(data_cleaned$in.sqft,
                                       breaks=c(min(data_cleaned$in.sqft), 1220, 2176, max(data_cleaned$in.sqft)),
                                       labels=c("Small", "Medium", "Large"),
                                       include.lowest=TRUE)
```

```{r}
# Impute missing values in the data_cleaned data frame
data_cleaned <- data_cleaned %>%
  # Impute missing values in numeric columns with their mean
  mutate_if(is.numeric, ~ ifelse(is.na(.), mean(., na.rm = TRUE), .)) %>%
  # Impute missing values in categorical columns with the mode (most frequent value)
  mutate_if(is.character, function(x) {
    # Identify the mode (most frequent value) for each categorical column
    mode_value <- names(sort(table(x), decreasing = TRUE))[1]
    # Replace missing values with the mode value
    ifelse(is.na(x), mode_value, x)
  })

```

```{r}
# Calculate Percentage of Missing Data
missing_percentage <- data_cleaned %>% 
  summarise(across(where(is.character), ~sum(is.na(.))/n()*100))

#View(missing_percentage)
# no missing categorical data

#Check for non-standard missing values and replace them with NA
data_cleaned <- data_cleaned %>% 
  mutate(across(where(is.character), ~na_if(., ""))) # Replace empty strings with NA

# Calculate the percentage of missing data for each column
missing_percentage <- colSums(is.na(data_cleaned)) / nrow(data_cleaned) * 100

# Identify columns where more than 70% of data is missing
columns_to_remove <- names(missing_percentage[missing_percentage > 70])

# Remove these columns from the dataframe
data_cleaned <- data_cleaned[, !(names(data_cleaned) %in% columns_to_remove)]

# Function to calculate the mode
get_mode <- function(v) {
  uniqv <- unique(v)
  uniqv[which.max(tabulate(match(v, uniqv)))]
}

# Replace NA in each column with its mode
data_cleaned <- data_cleaned %>%
  mutate(across(where(is.character), ~ifelse(is.na(.), get_mode(.), .)))

```

```{r}
# Save the data frame into a parquet file
write_parquet(data_cleaned, "/Users/mj/Desktop/IDS_Nikitha/data_cleaned.parquet")
```


####################################### Data Exploration ###########################################

```{r}
data_eda=read_parquet("/Users/mj/Desktop/IDS_Nikitha/data_cleaned.parquet")
```

```{r}
#Fuel Types Distribution Across Cities

ggplot(data_eda, aes(x = in.weather_file_city, fill = in.heating_fuel)) + 
  geom_bar() + 
  labs(title = "Fuel Types Distribution Across Cities", x = "City") + 
  guides(color = guide_legend(title = "Fuel Type")) +
  theme(plot.title = element_text(hjust = 0.5),axis.text.x = element_text(angle = 45, hjust = 1))
```


```{r}
#Energy Consumption in One and Two Story Buildings

ggplot(data_eda, aes(x = as.factor(in.geometry_stories), 
                      y = total_energy_consumed)) + 
  geom_boxplot() + 
  labs(title = "Energy Consumption in One and Two Story Buildings", 
       x = "Stories", y = "Energy Consumption") +
  theme(plot.title = element_text(hjust = 0.5))
```


```{r}
# Heatmap to visualize the correlations among weather and energy consumption

# Calculate the correlation matrix
library(reshape2)
weather_columns <- c(
  
  'total_Diffuse_Horizontal_Radiation',
  'total_Direct_Normal_Radiation',
  'median_Dry_Bulb_Temperature',
  'median_Relative_Humidity',
  'median_Wind_Speed',
  'median_Wind_Direction',
  'total_Global_Horizontal_Radiation'
)

weather_and_energy_df <- data_eda[, c(weather_columns, 'total_energy_consumed')]
correlation_matrix <- cor(weather_and_energy_df)

ggplot(data = melt(correlation_matrix), aes(Var1, Var2, fill = value)) +
  geom_tile() +
  scale_fill_gradient(low = "red", high = "yellow") +
  theme_minimal() +
  labs(x = "Variables", y = "Variables", fill = "Correlation") +
  ggtitle("Correlation Matrix Between Weather Data and Total Energy Usage") +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))
```



```{r}

# Average Temperature and Electricity Over Time (July 2018)

average_temp_by_day <- data_eda %>%
  summarize(average_temperature = mean(median_Dry_Bulb_Temperature, na.rm = TRUE))

average_electricity_by_day <- data_eda %>%
  summarize(average_electricity = mean(total_energy_consumed, na.rm = TRUE))

# Plot with two axes
ggplot() +
  geom_line(data = average_temp_by_day, aes(x = time, y = average_temperature, color = "Temperature"), size = 1) +
  geom_line(data = average_electricity_by_day, aes(x = time, y = average_electricity, color = "Electricity"), size = 1) +
  scale_color_manual(values = c("Temperature" = "blue", "Electricity" = "red")) +
  labs(x = "Date", y = "Temperature (°C)", color = "Variable") +
  ggtitle("Average Temperature and Electricity Over Time (July 2018)") +
  theme_minimal() +
  theme(
    axis.text.x = element_text(angle = 45, hjust = 1),
    legend.position = "bottom"
  ) +
  # Create a secondary y-axis for electricity
  scale_y_continuous(
    name = "Temperature (°C)",
    sec.axis = sec_axis(~ ., name = "Electricity (kWh)", 
                       labels = function(x) sprintf("%.0f", x))
  )

```


```{r}
install.packages("corrplot")
library(corrplot)

# Selecting relevant columns (numerical variables)
selected_data <- data_cleaned %>%
  select(total_energy_consumed, in.sqft, in.bedrooms, median_Dry_Bulb_Temperature)  # Add other numerical columns as needed

# Calculating the correlation matrix
correlation_matrix <- cor(selected_data, use = "complete.obs")

# Creating the correlation heatmap
corrplot(correlation_matrix, method = "color", type = "upper", order = "hclust", 
         tl.col = "black", tl.srt = 45, 
         addCoef.col = "black")  # Displays the correlation coefficient
```

```{r}
ggplot(data_cleaned, aes(x = factor(in.bedrooms), y = total_energy_consumed)) +
  geom_boxplot(fill = "lightblue") +
  theme_minimal() +
  labs(title = "Energy Consumption by Number of Bedrooms",
       x = "Number of Bedrooms",
       y = "Total Energy Consumed") +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))
```

```{r}
library(scales)

data_cleaned <- data_cleaned %>%
  mutate(sqft_range = cut(in.sqft,
                          breaks = c(0, 100, 500, 1000, 1500, 2000,3000,5000,6000,Inf),
                          labels = c("0-100", "101-500", "501-1000", "1001-1500", "1501-2000", "2001-3000","3001-5000","5001-6000","6000+"),
                          include.lowest = TRUE))

# Calculating the average energy consumed for each square footage range
avg_energy_by_sqft_range <- data_cleaned %>%
  group_by(sqft_range) %>%
  summarize(Average_Energy = mean(total_energy_consumed, na.rm = TRUE))

# Creating the bar graph
ggplot(avg_energy_by_sqft_range, aes(x = sqft_range, y = Average_Energy)) +
  geom_bar(stat = "identity", fill = "purple") +
  theme_minimal() +
  labs(title = "Average Energy Consumption by Square Footage Range",
       x = "Square Footage Range",
       y = "Average Energy Consumed") +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))
```

```{r}
# Plotting the data
ggplot(data_cleaned, aes(x=in.geometry_wall_type, y=total_energy_consumed)) +
  geom_bar(stat="identity", fill="coral") +
  theme_minimal() +
  labs(title="Average Total Energy Consumed by Wall Type",
       x="Wall Type",
       y="Average Total Energy Consumed")
```


```{r}

# Filter out rows with 'None' in the wall exterior type
filtered_data <- data_cleaned %>%
  filter(in.geometry_wall_exterior_finish != "None")

# Calculate average energy consumption for each wall exterior type
avg_energy_consumption <- filtered_data %>%
  group_by(in.geometry_wall_exterior_finish) %>%
  summarize(Average_Energy_Consumed = mean(total_energy_consumed, na.rm = TRUE))

# Create a bar graph
ggplot(avg_energy_consumption, aes(x = in.geometry_wall_exterior_finish, y = Average_Energy_Consumed)) +
  geom_bar(stat = "identity", fill = "steelblue") +
  theme(axis.text.x = element_text(angle = 45, hjust = 1)) +
  labs(title = "Average Energy Consumption by Wall Exterior Type",
       x = "Wall Exterior Type",
       y = "Average Energy Consumed")
```


```{r}
#Energy Consumption and Production by City

ggplot(data_eda, aes(x = in.weather_file_city, 
                     y = (total_energy_consumed-total_energy_produced))) + 
  geom_bar(stat = "identity", fill = "blue") +
  geom_line(aes(y = total_energy_produced), color = "red", size = 1) +
  labs(title = "Energy Consumption and Production by City", 
       x = "City", y = "Total Energy") +
  theme(plot.title = element_text(hjust = 0.5),
        axis.text.x = element_text(angle = 45, hjust = 1))

# we can see that the energy consumption in Daniel Field, Maxton, and Rutherfordton are low as they are good producers of renewable energy
```


####################################### Data Modelling ###########################################

```{r}
data_modelling <- read_parquet("/Users/mj/Desktop/IDS_Nikitha/data_cleaned.parquet")
```

```{r}
#Assuming that negative values occur because the house is generating more energy than it consumes, meaning it doesn't require any energy from our company. Therefore, converting every negative value to zero.
data_modelling$total_energy_consumed[data_modelling$total_energy_consumed < 0] <- 0
summary(data_modelling$total_energy_consumed)
```

```{r}
#Remove the unnecessary columns : Building_id, energy variables
data_modelling<- data_modelling[,-1]
data_modelling<- data_modelling[,-4]
```


```{r}
# Split the data into training and testing sets
set.seed(123)
trainIndex <- createDataPartition(data_modelling$total_energy_consumed , p = 0.8, list = FALSE)
train_data <- data_modelling[trainIndex, ]
test_data <- data_modelling[-trainIndex, ]
```


```{r}

# Linear Regression Model
lm <- lm(total_energy_consumed ~ ., data = train_data)

# Print summary
summary(lm)
```

```{r}
# Predicted values from the linear regression model
predicted_values <- predict(lm, newdata = test_data)

# Create a logical index to filter negative and NA values from Predicted Values
valid_index <- !is.na(predicted_values) & predicted_values >= 0
predicted_values <- predicted_values[valid_index]

# Actual values from the test data set
actual_values <- test_data$total_energy_consumed[valid_index]

# Calculate MAE, MSE, RMSE, and R-squared
MAE <- mean(abs(predicted_values - actual_values))
MSE <- mean((predicted_values - actual_values)^2)
RMSE <- sqrt(mean((predicted_values - actual_values)^2))
R_squared <- 1 - (sum((actual_values - predicted_values)^2) / sum((actual_values - mean(actual_values))^2))

# Print the metrics
cat("Mean Absolute Error (MAE):", MAE, "\n")
cat("Mean Squared Error (MSE):", MSE, "\n")
cat("Root Mean Squared Error (RMSE):", RMSE, "\n")
cat("R-squared (R²):", R_squared, "\n")

```


################################ Energy Consumption Prediction for 5 Degree warmer temperature ###########################################

```{r}
# Create a new dataset with temperatures increased by 5 units
data_5_warmer <- data_modelling
data_5_warmer$median_Dry_Bulb_Temperature <- data_5_warmer$median_Dry_Bulb_Temperature + 5

# Use the linear regression model to predict energy consumption with the modified dataset
predicted_energy_5_warmer <- predict(lm, newdata = data_5_warmer)

```


```{r}
# Read the cleaned data from a parquet file
data_cleaned <- read_parquet("/Users/mj/Desktop/IDS_Nikitha/data_cleaned.parquet")

# Create a copy of the cleaned data for further modifications
data_final <- data_cleaned

#Assuming that negative values occur because the house is generating more energy than it consumes, meaning it doesn't require any energy from our company. Therefore, converting every negative value to zero.
data_final$total_energy_consumed[data_final$total_energy_consumed < 0] <- 0
summary(data_final$total_energy_consumed)

# Add the predicted energy values for 5 degrees warmer to the data_final
data_final$predicted_energy_5_warmer <- predicted_energy_5_warmer

# Remove rows where predicted_energy_5_warmer is less than 0 or NA
data_final <- na.omit(data_final[!(is.na(data_final$predicted_energy_5_warmer) | data_final$predicted_energy_5_warmer < 0), ])

# Display a summary of the predicted_energy_5_warmer variable
summary(data_final$predicted_energy_5_warmer)

```

```{r}
# Save the final data frame into a parquet file
write_parquet(data_final, "/Users/mj/Desktop/IDS_Nikitha/data_final.parquet")
```














